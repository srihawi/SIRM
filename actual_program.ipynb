{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95076a6e",
   "metadata": {},
   "source": [
    "<font size=\"36\">1. Import Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66fc5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f0195e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic=mp.solutions.holistic\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_face_mesh=mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ceea25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB) #Color Conversion BGR 2 RGB\n",
    "    image.flags.writeable=False # Image is no longer writeable\n",
    "    results=model.process(image)    # Make prediction\n",
    "    image.flags.writeable=True  # Image is now writeable\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #Color Conversion RGB w BGR\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d283990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image,results):\n",
    "    mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96a1af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return np.concatenate([face])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b68296a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH=os.path.join('Training_Data')\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions=np.array(['active','bored','distracted'])\n",
    "\n",
    "#Thirty video worth of data\n",
    "no_sequences=30\n",
    "\n",
    "# Videos are going to be 30 franes in length\n",
    "sequence_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfc99dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efd55eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "face_prediction_model=keras.models.load_model('face_predictor.h5')\n",
    "engagement_prediction_model=keras.models.load_model('engagement_predictor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22f971a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database for the results\n",
    "import sqlite3\n",
    "\n",
    "connection = sqlite3.connect(\"engagement.db\")\n",
    "\n",
    "cur = connection.cursor()\n",
    "\n",
    "cur.execute( \"CREATE TABLE if not exists experiments (Experiment_id integer default 1,Experiment_time text, result_id integer primary key AUTOINCREMENT, StudentName text, EngagementType text)\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e190467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "result=cur.execute(\"select max(Experiment_id) from experiments\")\n",
    "\n",
    "\n",
    "experiment_id=cur.fetchone()[0]\n",
    "\n",
    "\n",
    "#experiment_id+=1\n",
    "if (experiment_id==None):\n",
    "    experiment_id=1\n",
    "    \n",
    "else:\n",
    "    experiment_id+=1\n",
    "\n",
    "print(experiment_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dbcc275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 325ms/step\n",
      "Face:(0):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Face:(1):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Face:(2):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Face:(0):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Face:(1):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Face:(0):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Face:(1):Mohamed\n",
      "distracted\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Face:(2):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Face:(0):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Face:(1):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Face:(2):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Face:(0):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Face:(1):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Face:(2):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Face:(0):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Face:(1):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Face:(2):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Face:(0):Mohamed\n",
      "active\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Face:(1):Mohamed\n",
      "active\n"
     ]
    }
   ],
   "source": [
    "sequence = []\n",
    "\n",
    "# Create a folder to store the main folder of the faces\n",
    "ct=datetime.datetime.now()\n",
    "main_folder_path='Experiment '+ct.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "os.mkdir(main_folder_path)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "\n",
    "cap = cv2.VideoCapture('Videos/Experiment 2.mp4')\n",
    "\n",
    "\n",
    "# read from the pickle file\n",
    "ResultMap=pd.read_pickle(r'ResultsMap.pkl')\n",
    "\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    face_mesh=mp_face_mesh.FaceMesh(max_num_faces=1,static_image_mode=True,refine_landmarks=True,min_detection_confidence=0.5,min_tracking_confidence=0.5) \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # Read feed\n",
    "        ret, frames = cap.read()\n",
    "        \n",
    "        # Close the video automatically\n",
    "        if (ret is False):\n",
    "            break\n",
    "     \n",
    "        gray=cv2.cvtColor(frames,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect the faces\n",
    "        faces=faceCascade.detectMultiScale(gray,scaleFactor=1.3,minNeighbors=2,minSize=(30,30))\n",
    "        \n",
    "        xcoords=[]\n",
    "        ycoords=[]\n",
    "        \n",
    "        \n",
    "        ct=datetime.datetime.now()\n",
    "        faces_folder_path=main_folder_path+'/Faces'+ct.strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "        os.mkdir(faces_folder_path)\n",
    "        \n",
    "        counter=0\n",
    "        for (x,y,w,h) in faces:\n",
    "            face = frames[y:y+h, x:x+w] #slice the face from the image\n",
    "            # save each image in a separate file\n",
    "            cv2.rectangle(frames, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.imwrite(faces_folder_path+'/face'+str(counter)+'.jpg',face)\n",
    "            \n",
    "            # Add the positions of the faces into a list\n",
    "            xcoords.append(x)\n",
    "            ycoords.append(y)\n",
    "            \n",
    "            counter=counter+1\n",
    "\n",
    "           #########################################################################\n",
    "\n",
    "        '''########### Making single predictions ###########'''\n",
    "        for c in range(counter):\n",
    "            ImagePath=faces_folder_path+'/face'+str(c)+'.jpg'\n",
    "            img=cv2.imread(ImagePath)\n",
    "            \n",
    "            test_image=keras.preprocessing.image.load_img(ImagePath,target_size=(64, 64))\n",
    "            test_image=keras.preprocessing.image.img_to_array(test_image)\n",
    "\n",
    "            test_image=np.expand_dims(test_image,axis=0)\n",
    "\n",
    "            face_prediction_result=face_prediction_model.predict(test_image,verbose=0)\n",
    "            \n",
    "            #########################################################################\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(img, holistic)\n",
    "            #print(results)\n",
    "\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(img, results)\n",
    "            \n",
    "            # Save to the file\n",
    "            #cv2.imwrite(faces_folder_path+'/face'+str(c)+'.jpg',img)\n",
    "            cv2.imwrite(faces_folder_path+'/face'+str(c)+'.jpg',img)\n",
    "\n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            \n",
    "            #print(actions[np.argmax(res)])\n",
    "          \n",
    "            if len(sequence) == 30:\n",
    "                face_prediction_result=face_prediction_model.predict(test_image,verbose=0)\n",
    "                res = engagement_prediction_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(\"Face:(\" +str(c)+\"):\"+ResultMap[np.argmax(face_prediction_result)])                \n",
    "                print(actions[np.argmax(res)])\n",
    "                \n",
    "                \n",
    "                #cv2.putText(frames,ResultMap[np.argmax(face_prediction_result)]+ ' '+actions[np.argmax(res)], (xcoords[c],ycoords[c]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frames,ResultMap[np.argmax(face_prediction_result)]+ ' '+actions[np.argmax(res)], (xcoords[c],ycoords[c]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "                                \n",
    "                # Add the prediction results to the database\n",
    "                #print(\"INSERT INTO experiments (Experiment_id,Experiment_time,StudentName,EngagementType ) values(\"+str(experiment_id)+\",'\"+ct.strftime(\"%m-%d-%Y-%H-%M-%S\")+\"','\"+ResultMap[np.argmax(face_prediction_result)]+\"','\"+actions[np.argmax(res)]+\"')\")\n",
    "                \n",
    "                cur.execute(\"INSERT INTO experiments (Experiment_id,Experiment_time,StudentName,EngagementType ) values(\"+str(experiment_id)+\",'\"+ct.strftime(\"%m-%d-%Y-%H-%M-%S\")+\"','\"+ResultMap[np.argmax(face_prediction_result)]+\"','\"+actions[np.argmax(res)]+\"')\")\n",
    "                connection.commit()\n",
    "                \n",
    "                \n",
    "            # Show to screen\n",
    "            cv2.namedWindow(\"Main Window\", cv2.WINDOW_NORMAL)\n",
    "            cv2.resizeWindow(\"Main Window\", 900, 500)\n",
    "            cv2.imshow('Main Window', frames)\n",
    "            \n",
    "            \n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(1000) & 0xFF == ord('q'):\n",
    "            break\n",
    "    connection.close\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355787f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd50171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
